<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>
  </head>
  <body>
    <h1>
      Сравнение открытых OLAP-систем Big Data: ClickHouse, Druid и&nbsp;Pinot
    </h1>

    <p>
      <a href="#">ClickHouse</a>, <a href="#">Druid</a> и
      <a href="#">Pinot</a>&nbsp;&mdash; три открытых хранилища данных, которые
      позволяют выполнять аналитические запросы на&nbsp;больших объемах данных
      с&nbsp;интерактивными задержками. Эта статья&nbsp;&mdash; перевод
      <a href="#">подробного сравнения</a>, выполненного Романом Левентовым.
    </p>
    <h3>Источники информации</h3>
    <p>
      Подробности реализации <b>ClickHouse</b> стали мне известны от
      <a href="#">Алексея Зателепина</a>, одного из
      <b>ключевых разработчиков проекта</b>. Доступная на&nbsp;английском
      документация достаточно скудна&nbsp;&mdash; наилучшим источником
      информации служат последние четыре секции
      <a href="#">данной страницы документации</a>.
    </p>
    <p>
      <b>Я&nbsp;сам участвую в&nbsp;развитии Druid</b>, но&nbsp;у&nbsp;меня нет
      личной заинтересованности в&nbsp;этой системе&nbsp;&mdash; по&nbsp;правде
      говоря, скорее всего в ближайшее время я&nbsp;перестану заниматься
      её&nbsp;разработкой. Поэтому читатели могут рассчитывать
      на&nbsp;отсутствие какой-либо предвзятости.
    </p>
    <p>
      Всё, что я&nbsp;буду далее писать про <b>Pinot</b>, основывается
      на&nbsp;странице <a href="#">Архитектура в&nbsp;вики Pinot</a>,
      а&nbsp;также на&nbsp;других страницах вики в&nbsp;разделе &laquo;Проектная
      документация&raquo;. Последний раз они обновлялись в&nbsp;июне 2017
      года&nbsp;&mdash; больше, чем полгода назад.
    </p>
    <p>
      Рецензентами оригинальной статьи стали Алексей Зателепин и
      <a href="#">Виталий Людвиченко</a> (разработчики ClickHouse),
      <a href="#">Жан Мерлино</a>
      (самый активный разработчик Druid),
      <a href="#">Кишор Гопалакришна</a> (архитектор Pinot) и
      <a href="#">Жан-Француа Им</a>
      (разработчик Pinot). Мы&nbsp;присоединяемся к&nbsp;благодарности автора
      и&nbsp;полагаем, что это многократно повышает авторитетность статьи.
    </p>
    <p>
      <b>Предупреждение</b>: статья достаточно большая, поэтому вполне возможно
      вы&nbsp;захотите ограничиться прочтением раздела &laquo;Заключение&raquo;
      в конце.
    </p>
    <h2>Сходства между системами</h2>
    <h3>Связанные данные и&nbsp;вычисления</h3>
    <p>
      <b
        >На&nbsp;фундаментальном уровне, ClickHouse, Druid и&nbsp;Pinot
        похожи</b
      >, поскольку они хранят данные и&nbsp;выполняют обработку запросов
      на&nbsp;одних и&nbsp;тех же&nbsp;узлах, уходя
      от&nbsp;&laquo;разъединенной&raquo; архитектуры BigQuery. Недавно
      я&nbsp;уже описывал несколько наследственных проблем со&nbsp;связанной
      архитектурой в&nbsp;случае Druid [<a href="#">1</a>, <a href="#">2</a>].
      Открытого эквивалента для BigQuery на&nbsp;данный момент
      не&nbsp;существует (за исключением, разве что, <a href="#">Drill</a>?)
      Возможным подходам к построению подобных открытых систем посвящена
      <a href="#">другая статье в&nbsp;моем блоге</a>.
    </p>
    <h3>
      Отличия от&nbsp;Big Data SQL-систем: индексы и&nbsp;статическое
      распределение данных
    </h3>
    <p>
      Рассматриваемые в&nbsp;этой статье системы
      <b>выполняют запросы быстрее</b>, чем системы Big Data из&nbsp;семейства
      класса SQL-on-Hadoop: Hive, Impala, Presto и&nbsp;Spark, даже когда
      последние получают доступ к&nbsp;данным, хранящимся в&nbsp;колоночном
      формате&nbsp;&mdash; к&nbsp;примеру, Parquet или Kudu. Это происходит
      потому, что в&nbsp;ClickHouse, Druid и&nbsp;Pinot:
    </p>
    <ul>
      <li>
        Имеется
        <b>свой собственный формат для хранения данных с&nbsp;индексами</b>,
        и&nbsp;они тесно интегрированы с&nbsp;движками обработки запросов.
        Системы класса SQL-on-Hadoop обычно можно назвать агностиками
        относительно форматов данных и&nbsp;поэтому они менее
        &laquo;навязчивы&raquo; в&nbsp;бэкендах Big Data.
      </li>
      <li>
        <b>Данные распределены относительно &laquo;статично&raquo;</b> между
        узлами, и&nbsp;при распределенном выполнении запроса это можно
        использовать. Обратная сторона медали при этом в&nbsp;том, что
        ClickHouse, Druid и&nbsp;Pinot
        <b
          >не&nbsp;поддерживают запросы, которые требуют перемещения большого
          количества данных</b
        >
        между узлами&nbsp;&mdash; к&nbsp;примеру, join между двумя большими
        таблицами.
      </li>
    </ul>
    <h3>Отсутствие точечных обновлений и&nbsp;удалений</h3>
    <p>
      Находясь на&nbsp;другой стороне спектра баз данных, ClickHouse, Druid
      и&nbsp;Pinot
      <b>не&nbsp;поддерживают точечные обновления и&nbsp;удаления</b>,
      в&nbsp;противоположность колоночным системам вроде Kudu, InfluxDB
      и&nbsp;Vertica (?). Это даёт ClickHouse, Druid и&nbsp;Pinot возможность
      производить более эффективное колоночное сжатие и&nbsp;более агрессивные
      индексы, что означает
      <b>большую эффективность использования ресурсов</b> и&nbsp;быстрое
      выполнение запросов.
    </p>
    <p>
      Разработчики ClickHouse в&nbsp;Yandex планируют начать поддерживать
      <a href="#">обновления и&nbsp;удаления в&nbsp;будущем</a>,
      но&nbsp;я&nbsp;не&nbsp;уверен, будут&nbsp;ли это &laquo;настоящие&raquo;
      точечные запросы или обновления/удаления диапазонов данных.
    </p>
    <h3>Поглощение в&nbsp;стиле Big Data</h3>
    <p>
      Все три системы поддерживают потоковое поглощение данных из&nbsp;Kafka.
      Druid и Pinot поддерживают потоковую передачу данных стриминг в
      <a href="#">Лямбда-стиле</a> и&nbsp;пакетное поглощение одних
      и&nbsp;тех&nbsp;же данных. ClickHouse поддерживает пакетные вставки
      напрямую, поэтому ему не требуется отдельная система пакетного поглощения
      подобная той, что используется в&nbsp;Druid и&nbsp;Pinot. Если вас
      интересуют подробности, то&nbsp;их&nbsp;вы сможете найти далее.
    </p>
    <h3>Проверено на&nbsp;крупном масштабе</h3>
    <p>
      Все три системы проверены на&nbsp;работоспособность в&nbsp;крупных
      масштабах: в
      <a href="#">Yandex.Metrica работает кластер ClickHouse</a>, состоящий из
      примерно десятка тысяч ядер CPU. В&nbsp;Metamarkets используется
      <a href="#">кластер Druid аналогичного размера</a>. Один кластер Pinot в
      LinkedIn включает в&nbsp;себя &laquo;<a href="#">тысячи машин</a>&raquo;.
    </p>
    <h3>Незрелость</h3>
    <p>
      Все рассматриваемые в&nbsp;статье системы являются
      <b>незрелыми по&nbsp;меркам открытых enterprise-систем Big Data</b>.
      Однако, скорее всего они незрелы не&nbsp;более, чем среднестатистическая
      открытая система Big Data&nbsp;&mdash; но&nbsp;это совсем другая история.
      В ClickHouse, Druid и&nbsp;Pinot недостает некоторых очевидных оптимизаций
      и функциональности, и&nbsp;они кишат багами (насчет ClickHouse
      и&nbsp;Pinot я&nbsp;не&nbsp;уверен на&nbsp;все 100%, но&nbsp;не&nbsp;вижу
      причин, по&nbsp;которым они в&nbsp;этом плане были&nbsp;бы лучше Druid).
    </p>
    <p>Это плавно подводит нас к&nbsp;следующему важному разделу.</p>
    <h2>Про сравнение производительности и&nbsp;выбор системы</h2>
    <p>
      Я&nbsp;регулярно вижу в&nbsp;сети, как некоторые проводят сравнения систем
      больших данных: они берут набор своих данных, каким-либо образом
      &laquo;скармливают&raquo; его оцениваемой системе, а&nbsp;затем немедленно
      пытаются измерить производительность&nbsp;&mdash; сколько памяти или
      дискового пространства было занято, и&nbsp;насколько быстро выполнялись
      запросы. Причем понимание того, как устроены изнутри испытываемые ими
      системы, у них отсутствует. Затем, используя лишь подобные специфичные
      данные о производительности&nbsp;&mdash; иногда вместе со&nbsp;списками
      функциональности, которая им нужна и&nbsp;которая есть в&nbsp;системе
      на&nbsp;настоящий момент,&nbsp;&mdash; они в&nbsp;итоге делают свой выбор
      или, что еще хуже, выбирают написать свою собственную,
      &laquo;лучшую&raquo; систему с нуля.
    </p>
    <p>
      Такой подход мне кажется неправильным, по&nbsp;крайней мере
      он&nbsp;неприменим в отношении открытых OLAP-систем для Big Data. Задача
      создания системы Bid Data OLAP, которая смогла&nbsp;бы работать эффективно
      в&nbsp;большинстве сценариев использования и&nbsp;содержала&nbsp;бы все
      необходимые функции настолько велика, что я оцениваю ее&nbsp;реализацию
      как минимум в&nbsp;<b>100 человеко-лет</b>.
    </p>
    <p>
      На&nbsp;сегодня, ClickHouse, Druid и&nbsp;Pinot оптимизированы только для
      конкретных сценариев использования, которые требуются
      их&nbsp;разработчиком&nbsp;&mdash; и&nbsp;содержат по большей части лишь
      те&nbsp;функции, в&nbsp;которых нуждаются сами разработчики. Я&nbsp;могу
      гарантировать, что ваш случай обязательно &laquo;упрется&raquo;
      в&nbsp;те&nbsp;узкие места, с&nbsp;которыми разработчики рассматриваемых
      OLAP-систем еще не&nbsp;сталкивались&nbsp;&mdash; или&nbsp;же в&nbsp;те
      места, что их&nbsp;не&nbsp;интересуют.
    </p>
    <p>
      Не говоря уже о&nbsp;том, что упомянутый выше подход &laquo;забросить
      данные в&nbsp;систему, о которой вы&nbsp;ничего не&nbsp;знаете,
      и&nbsp;затем измерить её&nbsp;эффективность&raquo; весьма вероятно даст
      искаженный результат из-за серьезных &laquo;узких&raquo; мест, которые
      на&nbsp;самом деле могли&nbsp;бы быть исправлены простым изменением
      конфигурации, схемы данных или другим построением запроса.
    </p>
    <h3>CloudFlare: ClickHouse против Druid</h3>
    <p>
      Одним таким примером, хорошо иллюстрирующим описанную выше проблему,
      является пост Марека Вавруша о&nbsp;
      <a href="#">выборе между ClickHouse и&nbsp;Druid в&nbsp;Cloudflare</a>. Им
      потребовалось 4&nbsp;сервера ClickHouse (которые со&nbsp;временем
      превратились в&nbsp;9), и&nbsp;по&nbsp;их оценкам, для разворачивания
      аналогичной установки Druid им&nbsp;бы потребовались &laquo;сотни
      узлов&raquo;. Пусть Марек и&nbsp;признает, что
      <b>сравнение является нечестным</b>, поскольку Druid недостаёт
      &laquo;сортировки по&nbsp;первичному ключу&raquo;, он&nbsp;возможно даже
      не осознает, что достичь примерно того&nbsp;же самого эффекта в&nbsp;Druid
      возможно просто
      <a href="#"
        >установив правильный порядок измерений в&nbsp;&laquo;ingestion
        spec&raquo;</a
      >
      и&nbsp;произведя простую подготовку данных: обрезать значение колонки
      <code>__time</code> в&nbsp;Druid до&nbsp;некоей грубой детализации
      (к&nbsp;примеру, один час) и&nbsp;опционально добавить другую
      &laquo;длинно-типовую&raquo; колонку &laquo;precise_time&raquo;, если для
      некоторых запросов требуются более точные временные рамки. Да, это хак,
      но, как мы&nbsp;только что выяснили, и&nbsp;в&nbsp;Druid можно сортировать
      данные по&nbsp;какому-либо измерению перед <code>__time</code>, и&nbsp;это
      достаточно просто реализовать.
    </p>
    <p>
      Впрочем, я&nbsp;не&nbsp;стану спорить с&nbsp;их&nbsp;итоговым решением
      выбрать ClickHouse, поскольку на&nbsp;масштабе примерно
      в&nbsp;10&nbsp;узлов и&nbsp;для их&nbsp;нужд ClickHouse мне тоже кажется
      лучшим выбором, чем Druid. Но&nbsp;сделанное ими заключение о&nbsp;том,
      что ClickHouse как минимум на&nbsp;порядок эффективнее (по&nbsp;меркам
      стоимости инфраструктуры), чем Druid&nbsp;&mdash; это серьезное
      заблуждение. На&nbsp;самом деле, из&nbsp;рассматриваемых нами сегодня
      систем,
      <b
        >Druid предлагает наилучшую возможность для реально дешевых установок</b
      >
      (смотрите раздел &laquo;Уровни узлов обработки запросов в&nbsp;Druid
      &raquo; ниже).
    </p>
    <mark>
      Когда вы&nbsp;выбираете систему OLAP Big Data,
      не&nbsp;сравнивайте&nbsp;то, насколько они сейчас хорошо подходят для
      вашего случая. Сейчас они все субоптимальны. Вместо этого, сравните,
      насколько быстро ваша компания способна заставить двигаться эти системы
      в&nbsp;том направлении, которое нужно именно вам.
    </mark>
    <p>
      В&nbsp;силу своей фундаментальной архитектурной схожести, ClickHouse,
      Druid и&nbsp;Pinot имеют примерно один и&nbsp;тот&nbsp;же
      &laquo;предел&raquo; эффективности и&nbsp;оптимизации производительности.
      Здесь нет &laquo;волшебной таблетки&raquo;, которая позволила&nbsp;бы
      какой- либо из&nbsp;этих систем быть быстрее, чем остальные.
      Не&nbsp;позволяйте запутать себя тем фактом, что в&nbsp;своем текущем
      состоянии системы показывают себя очень по-разному в&nbsp;различных
      бенчмарках.
    </p>
    <p>
      Допустим, Druid не&nbsp;поддерживает &laquo;сортировку по&nbsp;первичному
      ключу&raquo; настолько хорошо, насколько это умеет ClickHouse&nbsp;&mdash;
      а&nbsp;ClickHouse в&nbsp;свою очередь не поддерживает
      &laquo;инвертированные индексы&raquo; столь&nbsp;же хорошо, как Druid, что
      дает данным системам преимущества с&nbsp;той или иной нагрузкой.
      <b
        >Упущенные оптимизации могут быть реализованы в&nbsp;выбранной системе
        при помощи не&nbsp;таких уж&nbsp;и больших усилий </b
      >, если у&nbsp;вас есть намерение и&nbsp;возможность решиться
      на&nbsp;подобный шаг.
    </p>
    <ul>
      <li>
        В вашей организации должны быть инженеры, способные прочитать, понять и
        модифицировать исходный код выбранной системы, к&nbsp;тому&nbsp;же
        у&nbsp;них должно быть на&nbsp;это время. Заметьте, что ClickHouse
        написан на&nbsp;C++, а&nbsp;Druid и&nbsp;Pinot &mdash; на&nbsp;Java.
      </li>
      <li>
        Или&nbsp;же ваша организация должна подписать контракт с&nbsp;компанией,
        которая оказывает поддержку выбранной системы. Это будут
        <a href="#">Altinity</a> для ClickHouse, <a href="#">Imply</a> и&nbsp;<a
          href="#"
          >Hortonworks</a
        >
        для Druid. Для Pinot таких компаний в&nbsp;данный момент нет.
      </li>
    </ul>
    <p>
      Другие сведения о&nbsp;разработке систем, которые вам стоит принять
      во&nbsp;внимание:
    </p>
    <ul>
      <li>
        Авторы ClickHouse, работающие в&nbsp;Yandex, утверждают, что они
        тратят&nbsp;50% своего времени на&nbsp;создание функциональности,
        которая требуется им&nbsp;внутри компании, и&nbsp;другие&nbsp;50% уходят
        на&nbsp;функции, который набирают большинство &laquo;голосов
        сообщества&raquo;. Однако, чтобы вы&nbsp;получили от&nbsp;этого факта
        преимущество, требуется, чтобы
        <b
          >функции, которые нужны вам, были и наиболее востребованы
          сообществом</b
        >
        ClickHouse.
      </li>
      <li>
        Разработчики Druid из&nbsp;Imply мотивированы работать над широко
        используемыми функциями, поскольку это позволит им&nbsp;максимально
        увеличить объем охвата своего бизнеса в&nbsp;будущем.
      </li>
      <li>
        Процесс разработки Druid сильно напоминает
        <a href="#">модель Apache</a>, когда&nbsp;ПО несколько лет
        разрабатывается несколькими компаниями, у&nbsp;каждой из&nbsp;который
        достаточно своеобразные и&nbsp;различные приоритеты, и&nbsp;среди них
        нет ведущей компании. ClickHouse и&nbsp;Pinot пока еще далеки
        от&nbsp;подобного этапа, поскольку ими занимаются соответственно лишь
        Yandex и&nbsp;Linkedin. Сторонний вклад в развитие Druid имеет
        минимальный шанс быть отклоненным в&nbsp;силу того, что он расходится
        с&nbsp;видением основного разработчика&nbsp;&mdash; ведь
        <b>в&nbsp;Druid нет &laquo;основной&raquo; компании-разработчика</b>.
      </li>
      <li>
        Druid поддерживает &laquo;API разработчика&raquo;, который позволяет
        привносить собственные типы колонок, механизмы агрегации, возможные
        варианты для &laquo;глубокого хранения&raquo; и&nbsp;пр., причем все это
        вы&nbsp;можете держать в&nbsp;кодовой базе, отдельной от&nbsp;самого
        ядра Druid. Данное API документировано разработчиками Druid, и&nbsp;они
        следят за&nbsp;его совместимостью с&nbsp;предыдущими версиями. Однако,
        оно недостаточно &laquo;взрослое&raquo;, и&nbsp;ломается практически
        с&nbsp;каждым новым релизом Druid. Насколько мне известно,
        в&nbsp;ClickHouse и&nbsp;Pinot схожие API не поддерживаются.
      </li>
      <li>
        Согласно Github,
        <b>над Pinot работает наибольшее число людей</b>&nbsp;&mdash;
        по&nbsp;всей видимости, лишь за&nbsp;прошлый год в&nbsp;Pinot было
        вложено <a href="#">не&nbsp;менее 10&nbsp;человеко-лет</a>. Для
        ClickHouse эта цифра составляет примерно 6&nbsp;человеко-лет, а&nbsp;для
        Druid&nbsp;&mdash; 7. В&nbsp;теории, это должно означать, что Pinot
        улучшается быстрее всех остальных систем, которые мы&nbsp;рассматриваем.
      </li>
    </ul>
    <p>
      Архитектуры Druid и&nbsp;Pinot почти что идентичны друг другу,
      в&nbsp;то&nbsp;время как ClickHouse стоит слегка в&nbsp;стороне. Поэтому
      сначала мы&nbsp;сравним ClickHouse c&nbsp;&laquo;обобщенной&raquo;
      архитектурой Druid/Pinot, а&nbsp;затем обсудим мелкие различия между Druid
      и&nbsp;Pinot.
    </p>
    <h2>Различия между ClickHouse и&nbsp;Druid/Pinot</h2>
    <h3>Управление данными: Druid и&nbsp;Pinot</h3>
    <p>
      В&nbsp;Druid и&nbsp;Pinot, все данные в&nbsp;каждой &laquo;таблице&raquo;
      (как&nbsp;бы она не&nbsp;называлась в терминологии этих систем)
      разбиваются на&nbsp;указанное количество частей. По временой оси, данные
      обычно разделены с&nbsp;заданым интервалом. Затем эти части данных
      &laquo;запечатываются&raquo; индивидуально в&nbsp;самостоятельные
      автономные сущности, называемые &laquo;сегментами&raquo;. Каждый сегмент
      включает в&nbsp;себя метаданные таблицы, сжатые столбчатые данные
      и&nbsp;индексы.
    </p>
    <p>
      Сегменты хранятся в&nbsp;файловой системе хранилища &laquo;глубокого
      хранения&raquo; (например, HDFS) и&nbsp;могут быть загружены на&nbsp;узлы
      обработки запросов, но&nbsp;последние не&nbsp;отвечают
      за&nbsp;устойчивость сегментов, поэтому узлы обработки запросов могут быть
      заменены относительно свободно.
      <b>Сегменты не&nbsp;привязаны жестко к&nbsp;конкретным узлам</b> и могут
      быть загружены на&nbsp;те&nbsp;или другие узлы. Специальный выделенный
      сервер (который называется &laquo;координатором&raquo; в&nbsp;Druid
      и&nbsp;&laquo;контроллером&raquo; в&nbsp;Pinot, но&nbsp;я&nbsp;ниже
      обращаюсь к&nbsp;нему как к&nbsp;&laquo;мастеру&raquo;) отвечает
      за&nbsp;присвоение сегментов узлам, и перемещению сегментов между узлами,
      если потребуется.
    </p>
    <p>
      Это не&nbsp;противоречит тому, что я&nbsp;отмечал выше, все три системы
      имеют статическое распределение данных между узлами, поскольку загрузки
      сегментов и&nbsp;их перемещения в&nbsp;Druid&nbsp;&mdash; и&nbsp;насколько
      я&nbsp;понимаю в&nbsp;Pinot&nbsp;&mdash; являются дорогими операциями
      и&nbsp;потому не&nbsp;выполняются для каждой отдельной очереди,
      а&nbsp;происходят обычно раз в&nbsp;несколько минут/часов/дней.
    </p>
    <p>
      Метаданные сегментов хранятся в&nbsp;ZooKeeper&nbsp;&mdash; напрямую
      в&nbsp;случае Druid, и&nbsp;при помощи фреймворка Helix в&nbsp;Pinot.
      В&nbsp;Druid метаданные также хранятся в&nbsp;базе SQL, об этом будет
      подробнее в&nbsp;разделе &laquo;Различия между Druid и&nbsp;Pinot&raquo;.
    </p>
    <h3>Управление данными: ClickHouse</h3>
    <p>
      В&nbsp;ClickHouse нет &laquo;сегментов&raquo;, содержащих данные,
      попадающие в&nbsp;конкретные временные диапазоны. В&nbsp;нем нет
      &laquo;глубокого хранения&raquo; для данных, узлы в&nbsp;кластере
      ClickHouse также отвечают и&nbsp;за&nbsp;обработку запросов,
      и&nbsp;за&nbsp;постоянство/устойчивость данных, хранящихся на&nbsp;них.
      Так что вам <b>не&nbsp;потребуется HDFS</b> или облачное хранилище данных
      вроде Amazon S3.
    </p>
    <p>
      В&nbsp;ClickHouse имеются секционированные таблицы, состоящие
      из&nbsp;указанного набора узлов. Здесь нет &laquo;центральной
      власти&raquo; или сервера метаданных. Все узлы, между которыми разделена
      та&nbsp;или иная таблица, содержат полные, идентичные копииметаданных,
      включая адреса всех остальных узлов, на&nbsp;которых хранятся секции этой
      таблицы.
    </p>
    <p>
      Метаданные секционированной таблицы включают &laquo;весы&raquo; узлов для
      распределения свежезаписываемых данных&nbsp;&mdash; к&nbsp;примеру, 40%
      данных должны идти на&nbsp;узел A, 30% на узел&nbsp;B и&nbsp;30%
      на&nbsp;C.&nbsp;Обычно&nbsp;же распределение должно происходить
      равномерно, &laquo;перекоос&raquo;, как в&nbsp;этом примере, требуется
      только тогда, когда к&nbsp;секционированной таблице добавляется новый узел
      и&nbsp;нужно побыстрее заполнить его какими-либо данными.
      <b>Обновления этих &laquo;весов&raquo; должны выполняться вручную</b>
      администраторами кластера ClickHouse, или&nbsp;же автоматизированной
      системой, построенной поверх ClickHouse.
    </p>
    <h3>Управление данными: сравнение</h3>
    <p>
      Подход к&nbsp;управлению данными в&nbsp;ClickHouse проще, чем в&nbsp;Druid
      и&nbsp;Pinot: не&nbsp;требуется &laquo;глубокого хранилища&raquo;, всего
      один тип узлов, не&nbsp;требуется выделенного сервера для управления
      данными. Но&nbsp;подход ClickHouse приводит к&nbsp;некоторым трудностям,
      когда любая таблица данных вырастает настолько большой, что требуется
      ее&nbsp;разбиение между десятком или более узлов: коэффициент усиления
      запроса становится настолько&nbsp;же велик, насколько и&nbsp;фактор
      секционирования&nbsp;&mdash; даже для запросов, которые покрывают
      небольшой интервал данных:
    </p>
    <figure>
      <img src="./page7.png" alt="Segmented data layout in druid/pinot" />
      <figcaption>
        <i>Компромисс распределения данных в&nbsp;ClickHouse</i>
      </figcaption>
    </figure>
    <p>
      В&nbsp;примере, показанном на&nbsp;изображении выше, данные таблицы
      распределены между тремя узлами в&nbsp;Druid/Pinot, но&nbsp;запрос
      по&nbsp;малому интервалу данных обычно затрагивает лишь два из&nbsp;них
      (до&nbsp;той поры, пока интервал не&nbsp;пересечет пограничный интервал
      сегмента). В&nbsp;ClickHouse, любые запросы будут вынуждены затронуть
      триузлв&nbsp;&mdash; если таблица сегментирована между тремя узлами.
      В&nbsp;данном примере разница не&nbsp;выглядит настолько существенно,
      однако представьте себе, что случится, если число узлов достигнет
      100&nbsp;&mdash; в&nbsp;то&nbsp;время как фактор сегментирования
      по-прежнему может быть равен, например, 10&nbsp;в Druid/Pinot.
    </p>
    <p>
      Чтобы смягчить эту проблему, самый большой кластер ClickHouse
      в&nbsp;Яндексе, состоящий из&nbsp;сотен узлов, в&nbsp;действительности
      разбит на&nbsp;многие &laquo;под-кластеры&raquo; с несколькими десятками
      узлов в&nbsp;каждом. Кластер ClickHouse используется в&nbsp;работе с
      аналитикой по&nbsp;веб-сайтам, и&nbsp;каждая точка данных имеет измерение
      &laquo;ID&nbsp;вебсайта&raquo;. Существует жесткая привязка
      каждого&nbsp;ID сайта к&nbsp;конкретному под-кластеру, куда идут все
      данные для этого идентификатора сайта. Поверх кластера ClickHouse есть
      слой бизнес-логики, который управляет этим разделением данных при
      поглощении данных и&nbsp;выполнении запросов. К&nbsp;счастью,
      в&nbsp;их&nbsp;сценариях использования совсем немного запросов затрагивают
      несколько идентификаторов сайтов, и&nbsp;подобные запросы идут
      не&nbsp;от&nbsp;пользователей сервиса, поэтому у&nbsp;них нет жесткой
      привязки к&nbsp;реальному времени согласно соглашению об&nbsp;уровне
      услуг.
    </p>
    <p>
      Другим недостатком подхода ClickHouse является&nbsp;то, что, когда кластер
      растет очень быстро, данные не&nbsp;могут перебалансироваться
      автоматически без участия человека, который вручную поменяет
      &laquo;веса&raquo; узлов в&nbsp;разбиваемой таблице.
    </p>
    <h3>Уровни узлов обработки запросов в&nbsp;Druid</h3>
    <p>
      Управление данными при помощи сегментов &laquo;проще себе
      представить&raquo;&nbsp;&mdash; эта концепция хорошо ложится на&nbsp;наши
      когнитивные способности. Сами сегменты можно перемещать между узлами
      относительно просто. Эта две причины позволили Druid реализовать
      &laquo;разделение на&nbsp;уровни&raquo; узлов, занимающихся обработкой
      запросов: старые данные автоматически перемещаются на&nbsp;сервера
      с&nbsp;относительно большими дисками, но&nbsp;меньшим количеством памяти
      и&nbsp;CPU, что позволяет
      <b>значительно снизить стоимость большого рабочего кластера Druid</b>
      за&nbsp;счет замедления запросов к&nbsp;более старым данным.
    </p>
    <p>
      Эта функция позволяет Metamarkets экономить сотни тысяч долларов расходов
      на инфраструктуру Druid каждый месяц&nbsp;&mdash; в&nbsp;противовес тому
      варианту, если&nbsp;бы использовался &laquo;плоский&raquo; кластер.
    </p>
    <figure>
      <img src="./page9.png" alt="Segmented data layout in druid/pinot" />
      <figcaption>
        <i>Уровни узлов обработки запросов в&nbsp;Druid</i>
      </figcaption>
    </figure>
    <p>
      Насколько мне известно, в&nbsp;ClickHouse и&nbsp;Pinot пока еще нет
      похожей функциональности &mdash;&nbsp;предполагается, что все узлы
      в&nbsp;их&nbsp;кластерах одинаковы.
    </p>
    <p>
      В&nbsp;силу того, что архитектура Pinot весьма схожа с&nbsp;архитектурой
      Druid, как мне кажется, будет не&nbsp;слишком сложно добавить аналогичную
      функцию в&nbsp;Pinot. Тяжелее будет в случае с&nbsp;ClickHouse, поскольку
      для реализации данной функции крайне полезно использование концепта
      &laquo;сегментов&raquo;, однако это всё равно возможно.
    </p>
    <h3>Репликация данных: Druid и&nbsp;Pinot</h3>
    <p>
      Единицей репликации в&nbsp;Druid и&nbsp;Pinot является единичный сегмент.
      Сегменты реплицируются на&nbsp;уровне &laquo;глубокого хранения&raquo;
      (например, в&nbsp;три реплики на&nbsp;HDFS, или при помощи хранилища
      BLOB-объектов в&nbsp;Amazon&nbsp;S3), и&nbsp;на&nbsp;уровне обработки
      запросов: обычно и&nbsp;в&nbsp;Druid и&nbsp;в&nbsp;Pinot, каждый сегмент
      загружается на&nbsp;два различных узла. &laquo;Мастер&raquo;-сервер
      мониторит уровни репликации для каждого сегмента и загружает сегмент
      на&nbsp;какой-либо сервер, если фактор репликации падает ниже заданного
      уровня (например, если какой-либо из&nbsp;узлов перестаёт отвечать).
    </p>
    <h3>Репликация данных: ClickHouse</h3>
    <p>
      Единицей репликации в&nbsp;ClickHouse является секция таблицы
      на&nbsp;сервере (например, все данные из&nbsp;какой-либо таблицы,
      хранящиеся на&nbsp;сервере). Аналогично секционированию, репликация
      в&nbsp;ClickHouse является скорее &laquo;статической и конкретной&raquo;,
      чем &laquo;в&nbsp;облачном стиле&raquo;: несколько серверов знают, что они
      являются репликами друг друга (для некоторой конкретной таблицы;
      в&nbsp;случае другой таблицы, конфигурация репликации может отличаться).
      Репликация предоставляет и устойчивость, и&nbsp;доступность запросов.
      Когда повреждается диск на&nbsp;одном узле, данные не&nbsp;теряются,
      поскольку они хранятся еще и&nbsp;на&nbsp;другом узле. Когда
      какой-либоузел временно недоступен, запросы могут быть перенаправлены
      на&nbsp;реплику.
    </p>
    <p>
      В&nbsp;самом большом кластере ClickHouse в&nbsp;Яндексе есть два
      одинаковых набора узлов в различных дата-центрах, и&nbsp;они спарены.
      В&nbsp;каждой паре узлы являются репликами друг друга (используется фактор
      репликации, равный двум), и&nbsp;они расположены в различных дата-центрах.
    </p>
    <p>
      ClickHouse полагается на&nbsp;ZooKeeper для управления
      репликацией&nbsp;&mdash; поэтому, если вам не&nbsp;нужна репликация,
      то&nbsp;вам не&nbsp;нужен и&nbsp;ZooKeeper. Это означает, что ZooKeeper не
      потребуется и&nbsp;для ClickHouse, развернутого на&nbsp;одиночном узле.
    </p>
    <h3>Поглощение данных: Druid и&nbsp;Pinot</h3>
    <p>
      В&nbsp;Druid и&nbsp;Pinot узлы обработки запросов специализируются
      на&nbsp;загрузке сегментов и обслуживают запросы к&nbsp;данным
      в&nbsp;сегментах; они не&nbsp;занимаются накоплением новых данных
      и&nbsp;производством новых сегментов.
    </p>
    <p>
      Когда таблица может обновляться с&nbsp;задержкой в&nbsp;час или более,
      сегменты создаются при помощи движков пакетной обработки&nbsp;&mdash;
      к&nbsp;примеру, Hadoop или Spark. И&nbsp;в&nbsp;Druid, и&nbsp;в Pinot есть
      первоклассная поддержка Hadoop из&nbsp;коробки. Существует
      <a href="#"
        >сторонний плагин для поддержки индексации Druid в&nbsp;Spark</a
      >, но&nbsp;в&nbsp;данный момент официально он&nbsp;не поддерживается.
      Насколько мне известно, в&nbsp;Pinot такого уровня поддержки Spark пока
      нет, то&nbsp;есть вы&nbsp;должны быть готовы разобраться
      с&nbsp;интерфейсами Pinot и&nbsp;кодом, а затем самостоятельно написать
      код на&nbsp;Java/Scala, пусть это и&nbsp;не&nbsp;должно быть слишком
      сложно. (Впрочем, с&nbsp;момента публикации оригинальной статьи поддержка
      Spark в&nbsp;Pinot <a href="#">была внесена контрибьютором</a>).
    </p>
    <p>
      Когда таблица должна обновляться в&nbsp;реальном времени, здесь приходит
      на&nbsp;помощь идея &laquo;реалтаймовых&raquo; узлов, которые делают три
      вещи: принимает новые данные из Kafka (Druid поддерживает и&nbsp;другие
      источники), обслуживает запросы с&nbsp;недавними данными, создает сегменты
      в&nbsp;фоне и&nbsp;затем записывает их&nbsp;в&nbsp;&laquo;глубокое
      хранилище&raquo;.
    </p>
    <h3>Поглощение данных: ClickHouse</h3>
    <p>
      Тот факт, что ClickHouse не&nbsp;требуется готовить
      &laquo;сегменты&raquo;, содержащие все данные и попадающие в&nbsp;заданные
      временные интервалы, позволяет строить более простую архитектуру
      поглощения данных. ClickHouse не&nbsp;требуется ни&nbsp;пакетный движок
      обработки вроде Hadoop, ни&nbsp;&laquo;реалтаймовые&raquo; узлы. Обычные
      узлы ClickHouse&nbsp;&mdash; те&nbsp;же самые, что занимаются хранением
      данных и&nbsp;обслуживают запросы к&nbsp;ним&nbsp;&mdash; напрямую
      принимают пакетные записи данных.
    </p>
    <p>
      Если таблица разбита на&nbsp;сегменты, то&nbsp;узел, который принимает
      пакетную запись (например, 10к строк) распределяет данные согласно
      &laquo;весам&raquo; (смотрите раздел ниже). Строки записываются одним
      пакетом, который формирует небольшое &laquo;множество&raquo;. Множество
      немедленно конвертируется в&nbsp;колоночный формат. На&nbsp;каждом узле
      ClickHouse работает фоновый процесс, который объединяет наборы строк
      в&nbsp;еще большие наборы. Документация ClickHouse сильно завязана
      на&nbsp;принцип, известныйкак &laquo;MergeTree&raquo;, и&nbsp;подчеркивает
      схожесть его работы с&nbsp;<a href="#">LSM-деревом</a>, хотя меня это
      слегка смущает, поскольку данные не&nbsp;организованы
      в&nbsp;деревья&nbsp;&mdash; они лежат в&nbsp;плоском колончатом формате.
    </p>
    <h3>Поглощение данных: сравнение</h3>
    <p>
      Поглощение данных в&nbsp;Druid и&nbsp;Pinot является
      &laquo;тяжелым&raquo;: оно состоит из&nbsp;нескольких различных сервисов,
      и&nbsp;управление ими&nbsp;&mdash; это тяжелый труд.
    </p>
    <p>
      Поглощение данных в&nbsp;ClickHouse гораздо проще (что компенсируется
      сложностью управления &laquo;историческими&raquo; данными&nbsp;&mdash;
      т.е. данными не&nbsp;в&nbsp;реальном времени), но&nbsp;и здесь есть один
      момент: вы&nbsp;должны иметь возможность собирать данные в&nbsp;пакеты до
      самого ClickHouse. Автоматическое поглощение и&nbsp;пакетный сбор данных
      из&nbsp;<a href="#">Kafka доступно &laquo;из&nbsp;коробки&raquo;</a>,
      но&nbsp;если у&nbsp;вас используется другой источник данных
      в&nbsp;реальном времени (здесь подразумевается всё, что угодно,
      в&nbsp;диапазоне между инфраструктурой запросов, альтернативной Kafka,
      и&nbsp;стриминговых движков обработки, вплоть до различных HTTP-endpoint),
      то&nbsp;вам придется создать промежуточный сервис по&nbsp;сбору пакетов,
      или&nbsp;же внести код напрямую в&nbsp;ClickHouse.
    </p>
    <h3>Выполнение запроса</h3>
    <p>
      В&nbsp;<b>Druid и&nbsp;Pinot</b> имеется отдельный слой узлов, называемых
      <i>&laquo;брокерами&raquo;</i>, которые принимают все запросы
      к&nbsp;системе. Они определяют, к&nbsp;каким &laquo;историческим&raquo;
      (<i>содержащим данные не&nbsp;в&nbsp;реальном времени</i>) узлам обработки
      запросов должны быть отправлены подзапросы, основываясь
      на&nbsp;отображении сегментов в&nbsp;узлы, в&nbsp;которых сегменты
      загружаются. Брокеры хранят информацию об&nbsp;отображении в&nbsp;памяти.
      Брокер-узлы отправляют дальше подзапросы к&nbsp;узлам обработки запросов,
      и&nbsp;когда результаты этих подзапросов возвращаются, брокер объединяет
      их&nbsp;и&nbsp;возвращает финальный комбинированный результат
      пользователю.
    </p>
    <p>
      Я&nbsp;не&nbsp;берусь предполагать, зачем при проектировании Druid
      и&nbsp;Pinot было принято решение о&nbsp;введении еще одного типа узлов.
      Однако, теперь они кажутся их неотъемлемой частью, поскольку, когда общее
      количество сегментов в&nbsp;кластере начинает превышать десять миллионов,
      информация об&nbsp;отображении сегментов в узлы начинает занимать
      гигабайты памяти. Это очень расточительно&nbsp;&mdash; выделять столько
      много памяти на&nbsp;каждом узле для обработки запросов. Вот вам
      и&nbsp;еще один недостаток, который накладывается на&nbsp;Druid
      и&nbsp;Pinot их&nbsp;&laquo;сегментированной&raquo; архитектурой
      управления данными.
    </p>
    <p>
      В&nbsp;<b>ClickHouse</b> выделять отдельный набор узлов под &laquo;брокер
      запросов&raquo; обычно не требуется. Существует специальный, эфемерный
      <a href="#">&laquo;распределенный&raquo; тип таблицы</a> в ClickHouse,
      который может быть установлен на&nbsp;любом узле, и&nbsp;запросы
      к&nbsp;этой таблице будут делать все то&nbsp;же, за&nbsp;что отвечают
      брокер-узлы в&nbsp;Druid и&nbsp;Pinot. Обычно подобные эфемерные таблицы
      размещаются на&nbsp;каждом узле, который участвует в секционированной
      таблице, так что на&nbsp;практике каждый узел может быть &laquo;входной
      точкой&raquo; для запроса в&nbsp;кластер ClickHouse. Этот узел может
      выпускать необходимые подзапросы к&nbsp;другим секциями, обрабатывать свою
      часть запроса самостоятельно изатем объединять её&nbsp;с&nbsp;частичными
      результатами от&nbsp;других секций.
    </p>
    <p>
      Когда узел (или один из&nbsp;процессинговых узлов в&nbsp;ClickHouse, или
      брокер-узел в&nbsp;Druid и Pinot) выпускает подзапросы к&nbsp;другим,
      и&nbsp;один или несколько подзапросов по&nbsp;какой- либо причине
      заканчиваются неудачей, ClickHouse и&nbsp;Pinot обрабатывают эту ситуацию
      правильно: они объединяют результаты успешно выполненных подзапросов
      вместе, и всё равно возвращают частичный результат пользователю.
      <a href="#">Druid этой функции сейчас очень недостает</a>: если в&nbsp;нем
      выполнение подзапроса заканчивается неудачей, то неудачей закончится
      и&nbsp;весь запрос целиком.
    </p>
    <h3>ClickHouse vs. Druid или Pinot: Выводы</h3>
    <p>
      &laquo;Сегментированный&raquo; подход к&nbsp;управлению данными
      в&nbsp;Druid и&nbsp;Pinot против более простого управления данными
      в&nbsp;ClickHouse определяет многие аспекты систем. Однако, важно
      заметить, что это различие оказывает небольшое (или не&nbsp;оказывает
      вовсе) влияние на&nbsp;потенциальную эффективность сжатия (впрочем,
      история про компрессию для всех трех систем имеет печальный конец
      по&nbsp;нынешнему состоянию дел), или на&nbsp;скорость обработки запросов.
    </p>
    <p>
      <b>ClickHouse</b> похож на&nbsp;традиционные RDMBS, например, PostgreSQL.
      В&nbsp;частности, ClickHouse можно развернуть на&nbsp;всего один сервер.
      Если планируемый размер невелик&nbsp;&mdash; скажем, не&nbsp;больше
      порядка 100 ядер CPU для обработки запросов и&nbsp;1&nbsp;TB данных,
      я&nbsp;бы сказал, что ClickHouse имеет значительные преимущества перед
      Druid и Pinot в&nbsp;силу своей простоты и&nbsp;отсутствия необходимости
      в&nbsp;дополнительных типах узлов, таких как &laquo;мастер&raquo;,
      &laquo;узлы поглощения в&nbsp;реальном времени&raquo;,
      &laquo;брокеры&raquo;. На&nbsp;этом поле, ClickHouse соревнуется скорее
      с&nbsp;InfluxDB, чем с&nbsp;Druid или Pinot.
    </p>
    <p>
      <b>Druid and Pinot</b> похож на&nbsp;системы Big Data вроде HBase. Здесь
      в&nbsp;виду имеются не характеристики производительности,
      а&nbsp;зависимость от&nbsp;ZooKeper, зависимость от персистентного
      реплицируемого хранилища (к&nbsp;примеру, HDFS), сосредоточение внимания
      на&nbsp;устойчивости к&nbsp;отказам отдельных узлов, а&nbsp;также
      автономная работа и управление данными, не&nbsp;требующими постоянного
      внимания человека.
    </p>
    <p>
      Для широкого спектра приложений, ни&nbsp;ClickHouse, ни&nbsp;Druid или
      Pinot не&nbsp;являются очевидными победителями. В&nbsp;первую очередь,
      вы&nbsp;должны принимать во&nbsp;внимание вашу способность разобраться
      с&nbsp;исходным кодом системы, исправлять баги, добавлять новые функции
      и&nbsp;т.д. Это подробнее обсуждается в&nbsp;разделе &laquo;Про сравнение
      производительности и&nbsp;выбор системы&raquo;.
    </p>
    <p>
      Во-вторых, вам стоит взглянуть на&nbsp;таблицу ниже. Каждая ячейка
      в&nbsp;этой таблице описывает свойство приложения, которое позволит
      определить предпочтительную систему. Строки отсортированы
      не&nbsp;в&nbsp;порядке важности. Важность различных свойств может
      разниться от&nbsp;приложения к&nbsp;приложению, но&nbsp;в&nbsp;целом можно
      применить следующий подход: если ваше приложение соответствует
      подавляющему большинству строк со&nbsp;свойствами в&nbsp;одной
      из&nbsp;колонок, то&nbsp;относящаяся к&nbsp;ней система в вашем случае
      является предпочтительным выбором.
    </p>
    <table>
      <thead>
        <tr>
          <th>ClickHouse</th>
          <th>Druid или Pinot</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>В&nbsp;организации есть эксперты по&nbsp;C++</td>
          <td>В организации есть эксперты по&nbsp;Java</td>
        </tr>
        <tr>
          <td>Малый кластер</td>
          <td>Большой кластер</td>
        </tr>
        <tr>
          <td>Немного таблиц</td>
          <td>Много таблиц</td>
        </tr>
        <tr>
          <td>Один набор данных</td>
          <td>Несколько несвязанных наборов данных</td>
        </tr>
        <tr>
          <td>Таблицы и&nbsp;данные находятся в&nbsp;кластере перманентно</td>
          <td>
            Таблицы и&nbsp;наборы данных периодически появляются в&nbsp;кластере
            и&nbsp;удаляются из него
          </td>
        </tr>
        <tr>
          <td>
            Размер таблиц (и&nbsp;интенсивность запросов к&nbsp;ним) остается
            стабильным во времени
          </td>
          <td>Таблицы значительно растут и сжимаются</td>
        </tr>
        <tr>
          <td>
            Однородные запросы (их&nbsp;тип, размер, распределение
            по&nbsp;времени суток и&nbsp;т.д.)
          </td>
          <td>Разнородные запросы</td>
        </tr>
        <tr>
          <td>
            В&nbsp;данных есть измерение, по&nbsp;которому оно может быть
            сегментировано, и&nbsp;почти не&nbsp;выполняется запросов, которые
            затрагивают данные, расположенные в нескольких сегментах
          </td>
          <td>
            Подобного измерения нет, и&nbsp;запросы часто затрагивают данные,
            расположенные во&nbsp;всем кластере
          </td>
        </tr>
        <tr>
          <td>
            Облако не&nbsp;используется, кластер должен быть развернут
            на&nbsp;специфическую конфигурацию физических серверов
          </td>
          <td>Кластер развернут в&nbsp;облаке</td>
        </tr>
        <tr>
          <td>Нет существующих кластеров Hadoop или Spark</td>
          <td>
            Кластеры Hadoop или Spark уже существуют и&nbsp;могут быть
            использованы
          </td>
        </tr>
      </tbody>
    </table>
    <p>
      <b>Примечание</b>: ни&nbsp;одно из&nbsp;свойств выше не&nbsp;означает, что
      вы&nbsp;должны использовать соответствующую систему (системы), или
      избегать другую. К&nbsp;примеру, если планируется, что ваш кластер будет
      большим, это не&nbsp;значит, что вы&nbsp;обязательно должны рассматривать
      только Druid или Pinot, исключив ClickHouse. Скорее всего, в данной
      ситуации Druid или Pinot могут быть лучшим выбором, но&nbsp;другие
      полезные свойства могут перевесить чашу весов в&nbsp;сторону ClickHouse,
      который для некоторых приложений является оптимальным выбором даже для
      больших кластеров.
    </p>
    <h2>Различия между Druid и&nbsp;Pinot</h2>
    <p>
      Как уже не&nbsp;раз отмечалось в&nbsp;данной статье, Druid и&nbsp;Pinot
      имеют весьма похожие архитектуры. Есть несколько достаточно заметных
      особенностей, которые есть в одной системе и&nbsp;отсутствуют
      в&nbsp;другой, и&nbsp;областей, в&nbsp;которых каждая из&nbsp;систем
      развита гораздо сильнее другой. Тем не&nbsp;менее, всё, о&nbsp;чем
      я&nbsp;собираюсь упомянуть ниже, можно воспроизвести в&nbsp;другой
      системе, приложив разумное количество усилий.
    </p>
    <p>
      Между Druid и&nbsp;Pinot существует лишь
      <b>одно существенное различие</b>, которое слишком велико для того, чтобы
      от&nbsp;него избавились в&nbsp;обозримом будущем&nbsp;&mdash; это
      <b>реализация управления сегментами</b>
      в&nbsp;мастер-ноде. Кстати, разработчики обеих систем наверняка
      не&nbsp;хотели&nbsp;бы этого делать в&nbsp;любом случае, поскольку оба
      подхода имеют свои &laquo;за&raquo;
      и&nbsp;&laquo;против&raquo;&nbsp;&mdash; среди них нет такого, который
      был&nbsp;бы лучше.
    </p>
    <h3>Управление сегментами в&nbsp;Druid</h3>
    <p>
      Мастер-нода в&nbsp;Druid (и&nbsp;ни&nbsp;один из&nbsp;узлов в&nbsp;Pinot)
      не&nbsp;отвечают за&nbsp;сохранность метаданных в&nbsp;сегментах данных
      в&nbsp;кластере, и&nbsp;текущее отображение между сегментами и&nbsp;узлами
      обработки данных, на&nbsp;которых загружены сегменты. Эта информация
      хранится в&nbsp;ZooKeeper. Однако, Druid в&nbsp;дополнение хранит эту
      информацию еще и&nbsp;в&nbsp;SQL базе данных, которая необходима для
      развертывания кластера Druid. Не&nbsp;могу сказать, с&nbsp;какой целью
      было принято такое решение, но сейчас оно дает следующие преимущества:
    </p>
    <ul>
      <li>
        В ZooKeeper <b>хранится меньше данных</b>. Только минимум информации об
        отображении идентификатора сегмента на&nbsp;список узлов, занимающихся
        обработкой запросов, куда загружен сегмент, сохраняется
        в&nbsp;ZooKeeper. Оставшиеся метаданные, к&nbsp;примеру, размер
        сегмента, список измерений и метрики, и&nbsp;т.д. &mdash;&nbsp;хранятся
        только в&nbsp;SQL базе данных.
      </li>
      <li>
        Когда сегменты данных вытесняются из&nbsp;кластера, поскольку они
        становятся слишком старыми (это общая функция всех баз данных временных
        рядов&nbsp;&mdash; она есть и&nbsp;в&nbsp;ClickHouse,
        и&nbsp;в&nbsp;Druid, и&nbsp;в&nbsp;Pinot), они выгружаются из&nbsp;узлов
        обработки запросов и&nbsp;их&nbsp;метаданные удаляются
        из&nbsp;ZooKeeper, но&nbsp;не&nbsp;из&nbsp;&laquo;глубокого
        хранилища&raquo; и&nbsp;не&nbsp;из&nbsp;базы данных SQL. Пока они
        не&nbsp;будут удалены из&nbsp;этих мест вручную,
        <b
          >остается возможность &laquo;оживить&raquo; действительно старые
          данные быстро</b
        >, если он&nbsp;потребуются для построения отчетов или исследований.
      </li>
      <li>
        Вряд&nbsp;ли это планировалось с&nbsp;самого начала, но&nbsp;теперь есть
        планы сделать <b>зависимость Druid от&nbsp;ZooKeeper опциональной</b>.
        Сейчас ZooKeeper используется для трех различных функций: управления
        сегментами, обнаружения сервисов и&nbsp;хранения свойств (например, для
        управления поглощением данных в&nbsp;реальном времени). Обнаружение
        сервисов может <a href="#">быть предоставлено Consul</a>. Управление
        сегментами может быть реализовано
        <a href="#">при помощи HTTP-команд</a>, и&nbsp;оно доступно нам
        благодаря тому, что функции хранения в&nbsp;ZooKeeper
        &laquo;бекапится&raquo; в&nbsp;базе SQL.
      </li>
    </ul>
    <p>
      То&nbsp;что нам приходится иметь в&nbsp;зависимостях базу данных SQL,
      приводит к&nbsp;большей нагрузке на&nbsp;эксплуатацию, особенно, если
      в&nbsp;компании еще не&nbsp;использовалась какая- либо БД&nbsp;SQL. Druid
      поддерживает MySQL и&nbsp;PostgreSQL, есть и&nbsp;расширение для Microsoft
      SQL Server. Кроме того, когда Druid рразворачивается в&nbsp;облаке, можно
      использовать стандартные сервисы для управления RDBMS&nbsp;&mdash;
      к&nbsp;примеру, Amazon RDS.
    </p>
    <h3>Управление сегментами в&nbsp;Pinot</h3>
    <p>
      В&nbsp;отличие от&nbsp;Druid, который реализует всю логику управления
      сегментами самостоятельно и&nbsp;полагается только на&nbsp;<a href="#"
        >Curator</a
      >
      для взаимодействия с&nbsp;ZooKeeper, Pinot делегирует большую часть логики
      управления сегментами и&nbsp;кластерами на&nbsp;<a href="#"
        >фреймворк Helix</a
      >.
    </p>
    <p>
      С&nbsp;одной стороны, я&nbsp;могу понять, что это дает разработчикам Pinot
      возможность сосредоточиться на&nbsp;других частях их&nbsp;системы.
      В&nbsp;Helix возможно меньше багов, чем в логике внутри самого Druid,
      поскольку он&nbsp;тестируется в&nbsp;других условиях и&nbsp;поскольку в
      него, предположительно, было вложено гораздо больше рабочего времени.
    </p>
    <p>
      С&nbsp;другой стороны, Helix возможно ограничивает Pinot своими
      &laquo;рамками фреймворка&raquo;. Helix, и&nbsp;следовательно,
      <b>Pinot, скорее всего будут зависеть от&nbsp;ZooKeeper всегда</b>.
    </p>
    <p>
      Далее я&nbsp;собираюсь перечислить менее важные различия между Druid
      и&nbsp;Pinot&nbsp;&mdash; в&nbsp;том смысле, что если у&nbsp;вас возникнет
      серьезное желание повторить одну из&nbsp;этих функций в&nbsp;вашей
      системе, то&nbsp;это будет вполне осуществимо.
    </p>
    <h3>&laquo;Проталкивание предикатов&raquo; в&nbsp;Pinot</h3>
    <p>
      Если во&nbsp;время поглощения данные секционируются в&nbsp;Kafka
      по&nbsp;каким-либо ключам измерений, Pinot создает сегменты, которые
      содержат информацию об&nbsp;этом разбиении и&nbsp;затем, когда выполняется
      запрос с&nbsp;предикатом на&nbsp;данном измерении, брокер-узел фильтрует
      сегменты таким образом, чтобы как можно меньше сегментов и&nbsp;узлов
      обработки запросов было затронуто.
    </p>
    <p>
      Эта концепция в&nbsp;оригинале называется
      <b>&laquo;predicate pushdown&raquo;</b>
      и&nbsp;важна для поддержания высокой производительности в&nbsp;некоторых
      приложениях.
    </p>
    <p>
      На&nbsp;данный момент Druid поддерживает разбиение по&nbsp;ключам, если
      сегменты были созданы в&nbsp;Hadoop, но&nbsp;еще не&nbsp;поддерживает
      сегменты, созданные во&nbsp;время поглощения в&nbsp;реальном времени.
      Druid сейчас не&nbsp;реализует функцию &laquo;проталкивания
      предикатов&raquo; на&nbsp;брокеры.
    </p>
    <h3>&laquo;Сменный&raquo; Druid и&nbsp;своевольный Pinot</h3>
    <p>
      Поскольку Druid используют различные организации и&nbsp;в&nbsp;его
      разработке принимают участие несколько компаний, он&nbsp;обзавелся
      поддержкой нескольких взаимозаменяемых опций для практически любой
      выделенной части или &laquo;сервиса&raquo;:
    </p>
    <ul>
      <li>
        HDFS, Cassandra, Amazon S3, Google Cloud Storage или Azure Blob Storage
        и т.д.&nbsp;в&nbsp;качестве &laquo;глубокого хранилища&raquo;;
      </li>
      <li>
        Kafka, илиr RabbitMQ, Samza, или Flink, или Spark, Storm, и&nbsp;т.д.
        (через <a href="#">Tranquility</a>) в&nbsp;качестве источника поглощения
        данных в&nbsp;реальном времени;
      </li>
      <li>
        Сам Druid, или Graphite, или Ambari, или StatsD, или Kafka
        в&nbsp;качестве &laquo;слива&raquo; для телеметрии кластера Druid
        (метрик).
      </li>
    </ul>
    <p>
      В то&nbsp;же время Pinot почти целиком разрабатывался исключительно
      в&nbsp;стенах LinkedIn и должен был удовлетворять текущим нуждам компании,
      поэтому выбор, который вам предлагается, не&nbsp;так велик.
      В&nbsp;качестве &laquo;глубокого хранилища&raquo; необходимо использовать
      HDFS или Amazon&nbsp;S3, а&nbsp;для поглощения данных в&nbsp;реальном
      времени подойдет только Kafka. Но&nbsp;если кому-то это действительно
      понадобится, мне кажется, не&nbsp;составит особой сложности добавить
      поддержку любого другого сервиса в&nbsp;Pinot. К тому&nbsp;же, можно
      ожидать позитивных сдвигов в&nbsp;этом направлении, поскольку
      <a href="#">Uber</a> и Slack начинают использовать Pinot.
    </p>
    <h3>
      Формат данных и&nbsp;движок выполнения запросов лучше оптимизированы
      в&nbsp;Pinot
    </h3>
    <p>
      В&nbsp;частности, следующие функции
      <a href="#">формата сегментов Pinot</a> сейчас отсутствуют в Druid:
    </p>
    <ul>
      <li>
        <b>Сжатие проиндексированных столбцов</b> с&nbsp;битовой гранулярностью,
        но байтовой гранулярностью в&nbsp;Druid.
      </li>
      <li>
        <b>Инвертированный индекс опционален</b> для каждого столбца.
        В&nbsp;Druid он является обязательным, иногда этого не&nbsp;требуется,
        но&nbsp;все равно занимает много места. Различие в&nbsp;потреблении
        места между Druid и&nbsp;Pinot,
        <a href="#">на&nbsp;которое указывает Uber в&nbsp;своих тестах</a>,
        вполне возможно вызвано именно этим.
      </li>
      <li>
        <b>Минимальные и&nbsp;максимальные значения</b> в&nbsp;числовых столбцах
        записываются посегментно.
      </li>
      <li>
        <b>Поддержка сортировки данных из&nbsp;коробки</b>. В&nbsp;Druid этого
        можно достичь только вручную и&nbsp;слегка специфическим способом (как
        было описано в&nbsp;разделе &laquo;CloudFlare: ClickHouse против
        Druid&raquo;). Сортировка данных означает лучшее сжатие, и&nbsp;эта
        функция в&nbsp;Pinot&nbsp;&mdash; еще одна причина различия между Druid
        и&nbsp;Pinot в&nbsp;потреблении пространства (и&nbsp;производительности
        запросов!), на&nbsp;которую указывает Uber.
      </li>
      <li>
        <b>Формат данных</b>, используемый для многозначных столбцов,
        на&nbsp;данный момент лучше оптимизирован в&nbsp;Pinot, чем
        в&nbsp;Druid.
      </li>
    </ul>
    <p>
      Однако, все это можно реализовать и&nbsp;в&nbsp;Druid. И&nbsp;несмотря
      на&nbsp;то, что формат Pinot оптимизирован существенно лучше, чем формат
      Druid, он&nbsp;все равно достаточно далек от&nbsp;того, чтобы быть
      оптимальным. Один из&nbsp;примеров: Pinot (как и&nbsp;Druid) использует
      только сжатие общего назначение (как Zstd) и&nbsp;еще не&nbsp;реализовали
      идеи сжатия из Gorilla.
    </p>
    <p>
      К&nbsp;сожалению Uber по&nbsp;большей части использовал запросы
      <code>count (*)</code>
      для сравнения производительности Druid и&nbsp;Pinot относительно
      выполнения запроса [<a href="#">1</a>, <a href="#">2</a>], который сейчас
      в&nbsp;Druid представляет собой тупое линейное сканирование, хотя его
      и&nbsp;несложно заменить <a href="#">корректной O(1) реализацией</a>. Это
      вам еще один пример бессмысленных сравнений в&nbsp;стиле &laquo;черного
      ящика&raquo;, о&nbsp;которых мы&nbsp;говорили ранее.
    </p>
    <p>
      По моему мнению, причины сильного различия в&nbsp;производительности
      запросов <code>GROUP BY</code>, которое наблюдали в&nbsp;Uber, стоит
      искать в&nbsp;недостатке сортировки данных в сегментах Druid, как уже было
      отмечено выше в&nbsp;этом разделе.
    </p>
    <h3>
      У&nbsp;Druid есть более умный алгоритм присваивания (балансировки)
      сегментов
    </h3>
    <p>
      Алгоритм Pinot заключается в&nbsp;присвоении сегмента к&nbsp;узлам
      обработки запроса, которые имеют наименьшее число сегментов, загруженных
      в&nbsp;текущий момент. Алгоритм Druid является гораздо более сложным;
      он&nbsp;учитывает таблицу каждого сегмента и&nbsp;время, и&nbsp;применяет
      <b>сложную формулу для вычисления финального коэффициента</b>, согласно
      которому будут ранжированы узлы обработки запросов для выбора наилучшего,
      которому и&nbsp;будет присвоен новый сегмент. Этот алгоритм показал
      ускорение в&nbsp;скорости выполнения запросов в&nbsp;продакшне Metamarkets
      на&nbsp;<b>30- 40%</b>. Хотя, даже несмотря на&nbsp;подобный результат,
      мы&nbsp;им&nbsp;по-прежнему не&nbsp;слишком довольны&nbsp;&mdash;
      подробности можно прочитать <a href="#">в&nbsp;отдельной статье</a>.
    </p>
    <p>
      Не&nbsp;знаю, как в&nbsp;LinkedIn управляются со&nbsp;всем при помощи
      настолько простого алгоритма балансировки сегментов в&nbsp;Pinot, но,
      вполне возможно, их&nbsp;ожидают значительные улучшения по&nbsp;части
      производительности, если они решатся потратить время
      на&nbsp;совершенствование используемого ими алгоритма.
    </p>
    <h3>Pinot более устойчив к&nbsp;отказам при выполнении сложных запросов</h3>
    <p>
      Как уже упоминалось выше в&nbsp;разделе &laquo;Выполнение запроса&raquo;,
      когда брокер-узел создает подзапросы к&nbsp;другим узлам, некоторые
      подзапросы заканчиваются ошибкой, но&nbsp;Pinot объединяет результаты всех
      удачно выполненных подзапросов и&nbsp;по- прежнему возвращает частичный
      результат пользователю.
    </p>
    <p>В&nbsp;Druid такой функции на&nbsp;данный момент.</p>
    <h3>Иерархия узлов обработки запросов в&nbsp;Druid</h3>
    <p>
      Смотрите аналогичный раздел выше. Druid позволяет вводить уровни узлов
      обработки запросов для старых и&nbsp;новых данных, и&nbsp;для узлов
      со&nbsp;&laquo;старыми&raquo; данными соотношение &laquo;ресурсы CPU,
      RAM&nbsp;/ число загруженных сегментов&raquo; гораздо ниже, что позволяет
      выиграть на&nbsp;расходах на&nbsp;инфраструктуру в&nbsp;обмен
      на&nbsp;низкую производительность запросов при доступе к&nbsp;старым
      данным.
    </p>
    <p>
      Насколько мне известно, в&nbsp;Pinot на&nbsp;данный момент аналогичная
      функциональность отсутствует.
    </p>
    <h2>Заключение</h2>
    <p>
      <b
        >ClickHouse, Druid и&nbsp;Pinot имеют фундаментально схожую
        архитектуру</b
      >, и занимают свою собственную нишу между Big Data-фреймворками общего
      назначения вроде Impala, Presto, Spark, и&nbsp;колоночными базами данных
      с&nbsp;корректной поддержкой первичных ключей, точечных обновлений
      и&nbsp;удалений, как InfluxDB.
    </p>
    <p>
      В&nbsp;силу схожести архитектур, ClickHouse, Druid и&nbsp;Pinot имеют
      примерно одинаковый &laquo;предел оптимизации&raquo;. Но&nbsp;в&nbsp;своем
      текущем состоянии, все три системы еще незрелы и&nbsp;очень далеки
      от&nbsp;этого лимита. Существенных улучшений в производительности данных
      систем (применительно к&nbsp;специфическим сценариям использования) можно
      достичь несколькими человеко-месяцами работы опытных инженеров.
    </p>
    <mark>
      Я&nbsp;бы не&nbsp;рекомендовал вам сравнивать производительность данных
      систем между собой&nbsp;&mdash; выберите для себя&nbsp;ту, чей исходный
      код вы&nbsp;способны понять и модифицировать, или&nbsp;ту, в&nbsp;которую
      вы&nbsp;хотите инвестировать свои ресурсы.
    </mark>
    <p>
      Из&nbsp;этих трех систем, ClickHouse стоит немного в&nbsp;стороне
      от&nbsp;Druid и&nbsp;Pinot&nbsp;&mdash; в&nbsp;то&nbsp;время как Druid
      и&nbsp;Pinot практически идентичны, и&nbsp;их&nbsp;можно считать двумя
      независимо разрабатываемыми реализациями одной и&nbsp;той&nbsp;же системы.
    </p>
    <p>
      ClickHouse больше напоминает &laquo;традиционные&raquo; базы данных вроде
      PostgreSQL. ClickHouse можно установить на&nbsp;один узел. При малых
      масштабах (менее 1&nbsp;TB памяти, менее 100 ядер CPU), ClickHouse
      выглядит гораздо более интересным вариантом, чем Druid или
      Pinot&nbsp;&mdash; если вам все еще хочется
      их&nbsp;сравнивать&nbsp;&mdash; в&nbsp;силу того, что ClickHouse проще
      и&nbsp;имеет меньше движущихся частей и&nbsp;сервисов. Я&nbsp;бы даже
      сказал, что на&nbsp;таком масштабе он&nbsp;скорее становится конкурентом
      для InfluxDB или Prometheus, а&nbsp;не&nbsp;для Druid или Pinot.
    </p>
    <p>
      Druid и&nbsp;Pinot больше напоминают другие системы Big Data
      из&nbsp;экосистемы Hadoop. Они сохраняют свои
      &laquo;самоуправляемые&raquo; свойства даже на&nbsp;очень больших
      масштабах (более 500&nbsp;узлов), в&nbsp;то&nbsp;время как ClickHouse
      потребует для этого достаточно много работы профессиональных SRE. Кроме
      того, Druid и&nbsp;Pinot занимают выигрышную позицию в&nbsp;плане
      оптимизации инфраструктурной стоимости больших кластеров, и лучше подходят
      для облачных окружений, чем ClickHouse.
    </p>
    <p>
      Единственным долгосрочным различием между Druid и&nbsp;Pinot
      является&nbsp;то, что Pinot зависит от&nbsp;фреймворка Helix и&nbsp;будет
      продолжать зависеть от&nbsp;ZooKeeper, в&nbsp;то&nbsp;время как Druid
      может уйти от&nbsp;зависимости от&nbsp;ZooKeeper. С&nbsp;другой стороны,
      установка Druid продолжит зависеть от&nbsp;наличия какой-либо SQL-базы
      данных. На&nbsp;данный момент, Pinot оптимизирован лучше, чем Druid.
    </p>
    <mark>
      Если вы&nbsp;уже сталкивались с&nbsp;необходимостью сравнения этих систем
      и&nbsp;сделали свой выбор, то&nbsp;приходите на&nbsp;одну из&nbsp;наших
      конференций и&nbsp;расскажите о&nbsp;своем кейсе: о&nbsp;том какие именно
      были задачи и&nbsp;какие грабли (а&nbsp;наверняка они были) вы встретили.
      Хотя, конечно, базы данных далеко не&nbsp;единственная тема.
    </mark>
    <mark>
      Ближайший по&nbsp;окончанию срока подачи заявок
      (<b>до&nbsp;9&nbsp;апреля</b>) фестиваль <a href="#">РИТ++</a> включает
      направления: <a href="#">фронтенд</a>, <a href="#">бэкенд</a>,
      <a href="#">эксплуатацию</a> и&nbsp;<a href="#">управление</a>. Участникам
      обычно интереснее всего узнать о&nbsp;конкретных примерах, но&nbsp;и
      выступления и&nbsp;в&nbsp;виде обзоров и&nbsp;исследований тоже
      возможны&nbsp;&mdash; главное, чтобы тема была интересна лично вам.
    </mark>
  </body>
</html>
